# Retrieval-Augmented Generation (RAG) API Documentation

## 1. Architecture Overview

This system is built using a **Retrieval-Augmented Generation (RAG)** approach to enhance responses generated by a language model. The key components include:

- **LLM (TinyLlama/TinyLlama-1.1B-Chat-v1.0)**: Generates responses based on retrieved contextual data.
- **Embeddings (all-MiniLM-L6-v2)**: Converts documents and queries into vector embeddings for similarity search.
- **Vector Store (ChromaDB)**: Stores embeddings and retrieves relevant documents based on query similarity.
- **FastAPI**: Provides endpoints for document ingestion and query-based retrieval with response generation.

### Workflow

1. A document is uploaded via `/ingest`, converted into embeddings, and stored in ChromaDB.
2. A user query is submitted to `/query`, where:
    - The query is embedded using `all-MiniLM-L6-v2`.
    - Relevant documents are retrieved from ChromaDB based on similarity.
    - The retrieved texts and query are used as context for the LLM.
    - The LLM generates a concise response, avoiding redundancy.

---

## 2. External Tool Integration

The following external tools and libraries are integrated into this RAG system:

- **SentenceTransformer (`all-MiniLM-L6-v2`)**: For generating document and query embeddings.
- **ChromaDB**: A vector database used for storing and retrieving document embeddings.
- **Hugging Face Transformers (`TinyLlama/TinyLlama-1.1B-Chat-v1.0`)**: A text-generation model used to generate responses.
- **FastAPI**: Provides API endpoints for document ingestion and querying.
- **Uvicorn**: Used to run the FastAPI server.

---

## 3. Challenges & Resolutions

Several challenges were encountered while developing this system:

### 1. Model Selection for Response Generation

- **Issue**: Initial models like `gpt-neo` timed out frequently, and running larger models on a CPU was too slow.
- **Resolution**: Switched to `facebook/opt-350m`, but the output quality was poor. Finally, `TinyLlama/TinyLlama-1.1B-Chat-v1.0` was chosen as it balanced speed and response quality.

### 2. Handling Redundant and Duplicate Documents

- **Issue**: Ingested documents contained redundancy, and duplicate documents were being stored in ChromaDB.
- **Resolution**: Implemented a **threshold-based duplicate detection**. Documents were only stored if `existing_docs["ids"]` and `existing_docs["distances"][0][0] < 0.01`, ensuring only unique documents were retained.

### 3. Improving Response Quality

- **Issue**: The LLM was generating redundant or irrelevant responses.
- **Resolution**: Modified the prompt by explicitly adding "(concise, no redundancy)" to ensure clear and informative answers.

### 4. Efficient Query Processing

- **Issue**: Retrieving documents with similar but duplicate content.
- **Resolution**: Used `set()` to remove duplicate texts before passing them to the LLM for response generation.

### 5. Computational Constraints

- **Issue**: Running large models on a CPU was inefficient.
- **Resolution**: Selected a smaller, optimized model (`TinyLlama/TinyLlama-1.1B-Chat-v1.0`) that could generate responses with reasonable latency.

### **7. Latency and Efficiency**

- **Issue**: Query processing delays due to the initial model's computational inefficiency, creating a bottleneck in the retrieval pipeline. Redundant document retrieval further increased processing overhead.  
- **Resolution**: Upgraded to a more computationally efficient model, reducing latency. Streamlined the pipeline by optimizing duplicate handling, eliminating redundant computations, and enhancing overall system efficiency.

---

## 4. API Endpoints

### **Health Check**

- **Endpoint**: `GET /health`
- **Description**: Verifies API status.
- **Response**:
  ```json
  { "status": "OK" }
  ```

### **Document Ingestion**

- **Endpoint**: `POST /ingest`
- **Description**: Uploads and indexes a document (TXT or PDF) by generating embeddings.
- **Input**: File upload.
- **Response**:
  ```json
  { "message": "Document ingested successfully", "doc_id": "<ID>" }
  ```
  OR
  ```json
  { "message": "Duplicate document detected. Skipping ingestion." }
  ```

### **Query Processing**

- **Endpoint**: `GET /query`
- **Description**: Retrieves top-k relevant documents and generates a response using the LLM.
- **Input**: Query string (`query`) and number of documents to retrieve (`k`).
- **Response**:
```json
{
  "query": "What is deep learning?",
  "retrieved_documents": ["Deep learning is a subset of machine learning."],
  "generated_response": "Deep learning is a specialised subset of machine learning focused on neural networks."
}
```

---

This documentation provides a detailed view of the system architecture, external integrations, encountered challenges, and API functionality. The RAG approach ensures an efficient retrieval-augmented generation process, improving response accuracy and reliability.
